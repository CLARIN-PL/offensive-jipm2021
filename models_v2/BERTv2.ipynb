{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTv2-IPM",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAeuJ43DLQ4T"
      },
      "source": [
        "!pip install -U sentence-transformers --quiet\r\n",
        "!pip install pytorch-lightning==1.1.8 --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR_LdTJgLeg8"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import csv\r\n",
        "import urllib.request, zipfile, os\r\n",
        "import time\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "import pickle, gc\r\n",
        "\r\n",
        "import torch\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "%load_ext autoreload\r\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOLVmOaWLPDY"
      },
      "source": [
        "mkdir data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLNePv_Fzkg6"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPsghvcULMpC"
      },
      "source": [
        "if not os.path.exists('./data/attack_annotations.tsv'):\r\n",
        "  file_path = 'data/4054689.zip'\r\n",
        "  urllib.request.urlretrieve('https://ndownloader.figshare.com/articles/4054689/versions/6', file_path)\r\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\r\n",
        "      zip_ref.extractall('data')\r\n",
        "\r\n",
        "  file_path = 'data/4267550.zip'\r\n",
        "  urllib.request.urlretrieve('https://ndownloader.figshare.com/articles/4267550/versions/5', file_path)\r\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\r\n",
        "      zip_ref.extractall('data')\r\n",
        "\r\n",
        "  file_path = 'data/4563973.zip'\r\n",
        "  urllib.request.urlretrieve('https://ndownloader.figshare.com/articles/4563973/versions/2', file_path)\r\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\r\n",
        "      zip_ref.extractall('data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmTYje4GMi6j"
      },
      "source": [
        "aggression_data = pd.read_csv('./data/aggression_annotated_comments.tsv', sep='\\t')\r\n",
        "aggression_annotations = pd.read_csv('./data/aggression_annotations.tsv', sep='\\t')\r\n",
        "aggression_worker_demographics = pd.read_csv('/content/data/aggression_worker_demographics.tsv', sep='\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwf8sYd3oqDL"
      },
      "source": [
        "aggression_annotations = aggression_annotations.merge(aggression_worker_demographics)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T8Cz1QrzmlH"
      },
      "source": [
        "## Create text and annotator one hot feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFUKQ_cLxWVE"
      },
      "source": [
        "aggression_text_features = aggression_data.loc[:, ['year', 'logged_in', 'ns', 'sample']].fillna('empty')\r\n",
        "\r\n",
        "year_onehot = pd.get_dummies(aggression_text_features.year).values\r\n",
        "logged_in_onehot = pd.get_dummies(aggression_text_features.logged_in).values\r\n",
        "ns_onehot = pd.get_dummies(aggression_text_features.ns).values\r\n",
        "sample_onehot = pd.get_dummies(aggression_text_features['sample']).values\r\n",
        "\r\n",
        "text_features = np.hstack([year_onehot, logged_in_onehot, ns_onehot, sample_onehot])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqffPoPOvews"
      },
      "source": [
        "aggression_worker_demographics = aggression_worker_demographics.fillna('empty')\r\n",
        "\r\n",
        "worker_id_onehot = pd.get_dummies(aggression_worker_demographics.worker_id).values\r\n",
        "gender_onehot = pd.get_dummies(aggression_worker_demographics.gender).values\r\n",
        "english_first_language_onehot = pd.get_dummies(aggression_worker_demographics.english_first_language).values\r\n",
        "age_group_onehot = pd.get_dummies(aggression_worker_demographics.age_group).values\r\n",
        "education_onehot = pd.get_dummies(aggression_worker_demographics.education).values\r\n",
        "\r\n",
        "annotator_features = np.hstack([gender_onehot, english_first_language_onehot, age_group_onehot, education_onehot])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUPNr7OgeW35"
      },
      "source": [
        "## Bert embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOIP50V5MzIh"
      },
      "source": [
        "rev_ids = aggression_data.rev_id.to_list()\r\n",
        "comments = aggression_data.comment.to_list()\r\n",
        "comments = [c.replace('NEWLINE_TOKEN', ' ') for c in comments]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5k2g3HEMraF"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\r\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")\r\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwckm2LIN9wW"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "\r\n",
        "def get_embeddings(max_seq_len=256):\r\n",
        "  def batch(iterable, n=1):\r\n",
        "      l = len(iterable)\r\n",
        "      for ndx in range(0, l, n):\r\n",
        "          yield iterable[ndx:min(ndx + n, l)]\r\n",
        "\r\n",
        "  all_embeddings = []\r\n",
        "  for b_comments in tqdm(batch(comments, 200), total=len(comments)/200):\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "      batch_encoding = tokenizer.batch_encode_plus(\r\n",
        "            b_comments,\r\n",
        "            padding='longest',\r\n",
        "            add_special_tokens=True,\r\n",
        "            truncation=True, max_length=max_seq_len,\r\n",
        "            return_tensors='pt',\r\n",
        "        ).to(device)\r\n",
        "\r\n",
        "      emb = model(**batch_encoding)\r\n",
        "\r\n",
        "    for i in range(emb[0].size()[0]):\r\n",
        "      all_embeddings.append(emb[0][i, batch_encoding['input_ids'][i] > 0, :].mean(axis=0)[None, :])\r\n",
        "\r\n",
        "  return all_embeddings"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMT2Ml1RsrAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a58c07-5469-44c1-d83c-69fffec091bb"
      },
      "source": [
        "all_embeddings = get_embeddings(256)\n",
        "all_embeddings = torch.cat(all_embeddings, axis=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "580it [39:00,  4.03s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_mCjRFktg0G"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRGv8CTJ347O"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, classes_num=2, feature_num=768):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.feature_num = feature_num\r\n",
        "        self.fc1 = nn.Linear(feature_num, classes_num) \r\n",
        "\r\n",
        "    def forward(self, x, text_lengths=None):\r\n",
        "\r\n",
        "        x = x.view(-1, self.feature_num)\r\n",
        "        x = self.fc1(x)\r\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYlp8OXvth2q"
      },
      "source": [
        "import torch.utils.data as data\r\n",
        "\r\n",
        "class BatchIndexedDataset(data.Dataset):\r\n",
        "    def __init__(self, X, y, embeddings):\r\n",
        "        self.X = X\r\n",
        "        self.y = torch.tensor(y).long()\r\n",
        "        self.embeddings = embeddings\r\n",
        "\r\n",
        "        self.aggression_text_features = torch.tensor(text_features).to(device)\r\n",
        "        self.worker_id_onehot = torch.tensor(worker_id_onehot).to(device)\r\n",
        "        self.annotator_features = torch.tensor(annotator_features).to(device)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        revs_X = self.X[index, 0]\r\n",
        "        workers_X = self.X[index, 1]\r\n",
        "\r\n",
        "        batch_X = self.embeddings[revs_X]\r\n",
        "        batch_y = self.y[index]\r\n",
        "\r\n",
        "        if CFG['scenario'] == 's2':\r\n",
        "          batch_X = torch.cat([batch_X, self.annotator_features[workers_X], self.aggression_text_features[revs_X]], dim=1)\r\n",
        "\r\n",
        "        elif CFG['scenario'] == 's3':\r\n",
        "          batch_X = torch.cat([batch_X, self.annotator_features[workers_X], self.aggression_text_features[revs_X], self.worker_id_onehot[workers_X]], dim=1)\r\n",
        "\r\n",
        "        elif CFG['scenario'] == 's4':\r\n",
        "          negative_embeddings = annotator_negative_embeddings[workers_X].to(device)\r\n",
        "          positive_embeddings = annotator_positive_embeddings[workers_X].to(device)\r\n",
        "          batch_X = torch.cat([batch_X, self.annotator_features[workers_X], self.aggression_text_features[revs_X], negative_embeddings, positive_embeddings], dim=1)\r\n",
        "\r\n",
        "        return batch_X.float().to(device), batch_y.to(device)\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.y)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AF1WWWD0oTu"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\r\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\r\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\r\n",
        "import pytorch_lightning as pl\r\n",
        "from pytorch_lightning.metrics.functional import accuracy\r\n",
        "from pytorch_lightning import loggers as pl_loggers\r\n",
        "\r\n",
        "def prepare_dataloader(X, y):\r\n",
        "  dataset = BatchIndexedDataset(X, y, all_embeddings)        \r\n",
        "  sampler = data.sampler.BatchSampler(\r\n",
        "      data.sampler.RandomSampler(dataset),\r\n",
        "      batch_size=CFG['batch_size'],\r\n",
        "      drop_last=False)\r\n",
        "  \r\n",
        "  return data.DataLoader(dataset, sampler=sampler)\r\n",
        "\r\n",
        "def evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y):\r\n",
        "    \"\"\" Train classifier \"\"\"\r\n",
        "    train_loader = prepare_dataloader(train_X, train_y)\r\n",
        "    val_loader = prepare_dataloader(dev_X, dev_y)\r\n",
        "    test_loader = prepare_dataloader(test_X, test_y)\r\n",
        "\r\n",
        "    feature_num = next(iter(val_loader))[0].size(2)\r\n",
        "    model = HateClassifier(2, feature_num=feature_num).to(device)\r\n",
        "\r\n",
        "    tb_logger = pl_loggers.TensorBoardLogger('logs/')\r\n",
        "    trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0, max_epochs=CFG['epochs'], progress_bar_refresh_rate=20)\r\n",
        "    trainer.fit(model, train_loader, val_loader)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    model = model.to(device)\r\n",
        "    \r\n",
        "    test_probabs = [] \r\n",
        "    true_labels = []\r\n",
        "    with torch.no_grad():\r\n",
        "      for batch_text_X, batch_text_y in test_loader:\r\n",
        "        test_probabs.append(model(batch_text_X.to(device)))\r\n",
        "        true_labels.extend(batch_text_y.to(device).flatten().tolist())\r\n",
        "\r\n",
        "    test_probabs = torch.cat(test_probabs, dim=0)\r\n",
        "    test_predictions  = test_probabs.argmax(dim=1)\r\n",
        "\r\n",
        "    y_true = np.array(true_labels).flatten()\r\n",
        "    y_pred = test_predictions.tolist() \r\n",
        "\r\n",
        "    print(classification_report(y_true, y_pred))\r\n",
        "    result_dict = classification_report(y_true, y_pred, output_dict=True)\r\n",
        "    \r\n",
        "    return result_dict\r\n",
        "\r\n",
        "class HateClassifier(pl.LightningModule):\r\n",
        "    def __init__(self, classes_num=2, feature_num=768):\r\n",
        "        super().__init__()\r\n",
        "        self.model = Net(classes_num=classes_num, feature_num=feature_num).to(device)\r\n",
        "\r\n",
        "        self.train_acc = pl.metrics.Accuracy()\r\n",
        "        self.valid_acc = pl.metrics.Accuracy()\r\n",
        "        self.train_f1 = pl.metrics.F1(1,average=None)\r\n",
        "        self.valid_f1 = pl.metrics.F1(1, average=None)\r\n",
        "        self.valid_conf = pl.metrics.ConfusionMatrix(2)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.model(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def training_step(self, batch, batch_idx):\r\n",
        "        x, y = batch\r\n",
        "        y = y.flatten()\r\n",
        "        output = self.forward(x)\r\n",
        "\r\n",
        "        loss = nn.CrossEntropyLoss(torch.tensor(CFG['class_weights']).to(device))(output, y)\r\n",
        "        self.log('train_loss',  loss, on_epoch=True)\r\n",
        "        self.log('train_acc', self.train_acc(output, y), prog_bar=True)\r\n",
        "        self.log('train_f1', self.train_f1(output, y), prog_bar=True)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def training_epoch_end(self, outs):\r\n",
        "        epoch_acc = self.train_acc.compute()\r\n",
        "    \r\n",
        "    def validation_step(self, batch, batch_idx):\r\n",
        "        x, y = batch\r\n",
        "        y = y.flatten()\r\n",
        "        output = self.forward(x)\r\n",
        "        loss = nn.CrossEntropyLoss(torch.tensor(CFG['class_weights']).to(device))(output, y)\r\n",
        "\r\n",
        "        self.log('valid_loss', loss)\r\n",
        "        self.log('valid_acc', self.valid_acc(output, y), prog_bar=True)\r\n",
        "        self.log('valid_f1', self.valid_f1(output, y), prog_bar=True)\r\n",
        "        self.log('valid_conf', self.valid_conf(output, y))\r\n",
        "        \r\n",
        "        return {'loss': loss, 'true_labels': output, 'predictions': y}\r\n",
        "\r\n",
        "    def validation_epoch_end(self, outs):\r\n",
        "        val_epoch_acc = self.valid_acc.compute()\r\n",
        "        self.valid_f1.compute()\r\n",
        "        self.valid_conf.compute()\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=CFG['lr'])\r\n",
        "        return optimizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRu_Wff4zwbw"
      },
      "source": [
        "## Annotator personal embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exh9Qev_Rq9W"
      },
      "source": [
        "rev_id_idx_dict = aggression_data.loc[:, ['rev_id']].reset_index().set_index('rev_id').to_dict()['index']\r\n",
        "worker_id_idx_dict = aggression_worker_demographics.loc[:, ['worker_id']].reset_index().set_index('worker_id').to_dict()['index']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3hx7DA6TiY"
      },
      "source": [
        "train_X = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'train'].rev_id.values)].loc[:, ['rev_id', 'worker_id']]\r\n",
        "dev_X = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'dev'].rev_id.values)].loc[:, ['rev_id', 'worker_id']]\r\n",
        "test_X = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'test'].rev_id.values)].loc[:, ['rev_id', 'worker_id']]\r\n",
        "\r\n",
        "train_y = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'train'].rev_id.values)].aggression\r\n",
        "dev_y = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'dev'].rev_id.values)].aggression\r\n",
        "test_y = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'test'].rev_id.values)].aggression\r\n",
        "\r\n",
        "for df in [train_X, dev_X, test_X]:\r\n",
        "  df['worker_id'] = df['worker_id'].apply(lambda w_id: worker_id_idx_dict[w_id])\r\n",
        "  df['rev_id'] = df['rev_id'].apply(lambda r_id: rev_id_idx_dict[r_id])\r\n",
        "\r\n",
        "train_X, dev_X, test_X, train_y, dev_y, test_y = train_X.values, dev_X.values, test_X.values, train_y.values, dev_y.values, test_y.values"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVV88cq002EX"
      },
      "source": [
        "train_rev_ids = aggression_data[aggression_data.split == 'train'].rev_id.to_list()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9B-6bi4YtSE"
      },
      "source": [
        "annotator_negative_embeddings = torch.zeros(len(worker_id_idx_dict.keys()), 768)\r\n",
        "annotator_positive_embeddings = torch.zeros(len(worker_id_idx_dict.keys()), 768)\r\n",
        "\r\n",
        "worker_annotations = aggression_annotations[aggression_annotations.rev_id.isin(train_rev_ids)].groupby(['worker_id', 'aggression'])['rev_id'].apply(list).to_dict()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K_5c1y2YMdh"
      },
      "source": [
        "for i in worker_id_idx_dict.keys():\r\n",
        "  if (i, 0.0) in worker_annotations:\r\n",
        "    negative_text_idxs = [rev_id_idx_dict[r_idx] for r_idx in worker_annotations[(i, 0.0)]]\r\n",
        "    annotator_negative_embeddings[worker_id_idx_dict[i]] = all_embeddings[negative_text_idxs].mean(axis=0)\r\n",
        "  if (i, 1.0) in worker_annotations:\r\n",
        "    positive_text_idxs = [rev_id_idx_dict[r_idx] for r_idx in worker_annotations[(i, 1.0)]]\r\n",
        "    annotator_positive_embeddings[worker_id_idx_dict[i]] = all_embeddings[positive_text_idxs].mean(axis=0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_NGG45e2tnF"
      },
      "source": [
        "## S1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scfOh76FJxDR"
      },
      "source": [
        "CFG = {\r\n",
        "    'lr': 7*1e-4, \r\n",
        "    'epochs': 20,\r\n",
        "    'class_weights': [1.0, 1.0],\r\n",
        "    'batch_size': 1000,\r\n",
        "    'scenario': 's1'\r\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdQuh0wn4FKZ"
      },
      "source": [
        "results_s1 = {}\r\n",
        "for i in range(10):\r\n",
        "  results_s1[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4snRUzs2vVl"
      },
      "source": [
        "## S2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMIjrfgY2yG0"
      },
      "source": [
        "CFG = {\r\n",
        "    #'lr': 7*1e-4, \r\n",
        "    'lr': 7*1e-4, \r\n",
        "    'epochs': 20,\r\n",
        "    'class_weights': [1.0, 1.0],\r\n",
        "    'batch_size': 1000,\r\n",
        "    'scenario': 's2'\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrryoOGg229i"
      },
      "source": [
        "results_s2 = {}\r\n",
        "for i in range(10):\r\n",
        "  results_s2[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvdJDskSBncS"
      },
      "source": [
        "## S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7zwiO1GBo2w"
      },
      "source": [
        "CFG = {\r\n",
        "    #'lr': 7*1e-4, \r\n",
        "    'lr': 7*1e-4, \r\n",
        "    'epochs': 20,\r\n",
        "    'class_weights': [1.0, 1.0],\r\n",
        "    'batch_size': 1000,\r\n",
        "    'scenario': 's3'\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW2HOBe1Bqzr"
      },
      "source": [
        "results_s3 = {}\r\n",
        "for i in range(10):\r\n",
        "  results_s3[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v36pcQ5Jfy5s"
      },
      "source": [
        "## S4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af-lDK5wf0Am"
      },
      "source": [
        "CFG = {\r\n",
        "    #'lr': 7*1e-4, \r\n",
        "    'lr': 7*1e-4, \r\n",
        "    'epochs': 20,\r\n",
        "    'class_weights': [1.0, 1.0],\r\n",
        "    'batch_size': 1000,\r\n",
        "    'scenario': 's4'\r\n",
        "}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9rInicQDP2V"
      },
      "source": [
        "results_s4 = {}\r\n",
        "for i in range(10):\r\n",
        "  results_s4[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOm_qunAa6_v"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XegtNefXalio"
      },
      "source": [
        "def get_mean_results(results):\r\n",
        "  accuracy = np.mean([results[i]['accuracy'] for i in results.keys()])\r\n",
        "  precision_macro = np.mean([results[i]['macro avg']['precision'] for i in results.keys()])\r\n",
        "  recall_macro = np.mean([results[i]['macro avg']['recall'] for i in results.keys()])\r\n",
        "  f1_macro = np.mean([results[i]['macro avg']['f1-score'] for i in results.keys()])\r\n",
        "  precision_a = np.mean([results[i]['1']['precision'] for i in results.keys()])\r\n",
        "  recall_a = np.mean([results[i]['1']['recall'] for i in results.keys()])\r\n",
        "  f1_a = np.mean([results[i]['1']['f1-score'] for i in results.keys()])\r\n",
        "\r\n",
        "  return {'accuracy': accuracy, \r\n",
        "          'precision_macro': precision_macro,\r\n",
        "          'recall_macro': recall_macro,\r\n",
        "          'f1_macro': f1_macro,\r\n",
        "          'precision_a': precision_a,\r\n",
        "          'recall_a': recall_a,\r\n",
        "          'f1_a': f1_a,\r\n",
        "          }\r\n",
        "\r\n",
        "print('S1')\r\n",
        "print(get_mean_results(results_s1))\r\n",
        "\r\n",
        "print('S2')\r\n",
        "print(get_mean_results(results_s2))\r\n",
        "\r\n",
        "print('S3')\r\n",
        "print(get_mean_results(results_s3))\r\n",
        "\r\n",
        "print('S4')\r\n",
        "print(get_mean_results(results_s4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}