{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMv2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTr1CZeK7RK-"
      },
      "source": [
        "!pip install pytorch-lightning==1.1.8 --quiet\r\n",
        "!pip install fasttext==0.9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR_LdTJgLeg8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import urllib.request, zipfile, os\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle, gc\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOLVmOaWLPDY"
      },
      "source": [
        "mkdir data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eJWb65m7LWa"
      },
      "source": [
        "## Load dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ppTGb7R7gA7"
      },
      "source": [
        "if not os.path.exists('./data/attack_annotations.tsv'):\r\n",
        "  file_path = 'data/4054689.zip'\r\n",
        "  urllib.request.urlretrieve('https://ndownloader.figshare.com/articles/4054689/versions/6', file_path)\r\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\r\n",
        "      zip_ref.extractall('data')\r\n",
        "\r\n",
        "  file_path = 'data/4267550.zip'\r\n",
        "  urllib.request.urlretrieve('https://ndownloader.figshare.com/articles/4267550/versions/5', file_path)\r\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\r\n",
        "      zip_ref.extractall('data')\r\n",
        "\r\n",
        "  file_path = 'data/4563973.zip'\r\n",
        "  urllib.request.urlretrieve('https://ndownloader.figshare.com/articles/4563973/versions/2', file_path)\r\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\r\n",
        "      zip_ref.extractall('data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmTYje4GMi6j"
      },
      "source": [
        "aggression_data = pd.read_csv('./data/aggression_annotated_comments.tsv', sep='\\t')\n",
        "aggression_annotations = pd.read_csv('./data/aggression_annotations.tsv', sep='\\t')\n",
        "aggression_worker_demographics = pd.read_csv('./data/aggression_worker_demographics.tsv', sep='\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wkarCKY7LWb"
      },
      "source": [
        "aggression_data['comment_clean'] = aggression_data['comment'].str.replace('NEWLINE_TOKEN', ' ')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwf8sYd3oqDL"
      },
      "source": [
        "aggression_annotations = aggression_annotations.merge(aggression_worker_demographics)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsP81s0R7LWc"
      },
      "source": [
        "## Worker and text feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFUKQ_cLxWVE"
      },
      "source": [
        "aggression_text_features = aggression_data.loc[:, ['year', 'logged_in', 'ns', 'sample']].fillna('empty')\n",
        "\n",
        "year_onehot = pd.get_dummies(aggression_text_features.year).values\n",
        "logged_in_onehot = pd.get_dummies(aggression_text_features.logged_in).values\n",
        "ns_onehot = pd.get_dummies(aggression_text_features.ns).values\n",
        "sample_onehot = pd.get_dummies(aggression_text_features['sample']).values\n",
        "\n",
        "text_features = np.hstack([year_onehot, logged_in_onehot, ns_onehot, sample_onehot])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqffPoPOvews"
      },
      "source": [
        "aggression_worker_demographics = aggression_worker_demographics.fillna('empty')\n",
        "\n",
        "worker_id_onehot = pd.get_dummies(aggression_worker_demographics.worker_id).values\n",
        "gender_onehot = pd.get_dummies(aggression_worker_demographics.gender).values\n",
        "english_first_language_onehot = pd.get_dummies(aggression_worker_demographics.english_first_language).values\n",
        "age_group_onehot = pd.get_dummies(aggression_worker_demographics.age_group).values\n",
        "education_onehot = pd.get_dummies(aggression_worker_demographics.education).values\n",
        "\n",
        "annotator_features = np.hstack([gender_onehot, english_first_language_onehot, age_group_onehot, education_onehot])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cclzs8Dp7LWd"
      },
      "source": [
        "## Texts tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSmwYvJD7LWe"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=None, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(aggression_data.comment_clean.tolist())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCmctSHs7LWe"
      },
      "source": [
        "text_tokenized = tokenizer.texts_to_sequences(aggression_data.comment_clean.tolist())\n",
        "text_lens = [len(t) for t in text_tokenized]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_3mv1F47LWf"
      },
      "source": [
        "text_tokenized = pad_sequences(text_tokenized, maxlen=256, dtype='int32', padding='post', truncating='post', value=0.0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUPNr7OgeW35"
      },
      "source": [
        "## Fasttext load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkTJkF9CC7Ys",
        "scrolled": true
      },
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxeonRhaC8T_"
      },
      "source": [
        "ft = fasttext.load_model('cc.en.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKAD4Xni7LWh"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCr7eSoX7LWh"
      },
      "source": [
        "word_embeddings = torch.empty((len(tokenizer.word_index.keys()) + 1, 300))\n",
        "for w, i in tokenizer.word_index.items():\n",
        "    word_embeddings[i] = torch.tensor(ft[w])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJSS0tDe7LWh"
      },
      "source": [
        "all_embeddings = torch.empty((len(aggression_data.index), 300))\n",
        "texts = aggression_data.comment_clean.to_list()\n",
        "for i in range(len(aggression_data.index)):\n",
        "    all_embeddings[i] = torch.tensor(ft.get_sentence_vector(texts[i]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXUzEnpI7LWh"
      },
      "source": [
        "## Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRGv8CTJ347O"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, classes_num=2, feature_num=300):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.feature_num = feature_num\n",
        "        \n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(word_embeddings, \n",
        "                                            padding_idx=0)\n",
        "        \n",
        "        self.hidden_dim = 32\n",
        "        self.rnn = nn.LSTM(word_embeddings.shape[1], \n",
        "                           self.hidden_dim, \n",
        "                           num_layers=1, \n",
        "                           bidirectional=False, \n",
        "                           dropout=0.5, \n",
        "                           batch_first=True)\n",
        "        \n",
        "        self.fc1 = nn.Linear(self.hidden_dim + feature_num, classes_num)\n",
        "            \n",
        "    def forward(self, tokens, features):\n",
        "        x = self.embedding(tokens)\n",
        "\n",
        "        lens_X = (tokens != 0).sum(dim=1).to('cpu')\n",
        "        lens_X[lens_X == 0] = 1\n",
        "        \n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lens_X, batch_first=True, enforce_sorted=False).to(device)\n",
        "        \n",
        "        x, (hidden, cell) = self.rnn(x)\n",
        "        x = torch.cat([hidden.view(-1, self.hidden_dim), features.view(features.size(-2), self.feature_num)], dim=1)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_mCjRFktg0G"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1YukZJi7LWj"
      },
      "source": [
        "import torch.utils.data as data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from time import time\n",
        "\n",
        "class BatchIndexedDataset(data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = torch.tensor(y).long()\n",
        "\n",
        "        self.aggression_text_features = torch.tensor(text_features)#.to(device)\n",
        "        self.worker_id_onehot = torch.tensor(worker_id_onehot)#.to(device)\n",
        "        self.annotator_features = torch.tensor(annotator_features)#.to(device)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        revs_X = self.X[index, 0]\n",
        "        workers_X = self.X[index, 1]\n",
        "        \n",
        "        text_tokens = torch.tensor(text_tokenized[revs_X]).long()\n",
        "        \n",
        "        batch_X = text_tokens\n",
        "        batch_features = torch.empty((len(index), 0))\n",
        "        batch_y = self.y[index]\n",
        "        \n",
        "        if CFG['scenario'] == 's2':\n",
        "          batch_features = torch.cat([self.annotator_features[workers_X], self.aggression_text_features[revs_X]], dim=1)\n",
        "\n",
        "        elif CFG['scenario'] == 's3':\n",
        "          batch_features = torch.cat([self.annotator_features[workers_X], self.aggression_text_features[revs_X], self.worker_id_onehot[workers_X]], dim=1)\n",
        "          #batch_X = torch.cat([batch_X, self.worker_id_onehot[workers_X]], dim=1)\n",
        "\n",
        "        elif CFG['scenario'] == 's4':\n",
        "          negative_embeddings = annotator_negative_embeddings[workers_X]#.to(device)\n",
        "          positive_embeddings = annotator_positive_embeddings[workers_X]#.to(device)\n",
        "          batch_features = torch.cat([self.annotator_features[workers_X], self.aggression_text_features[revs_X], negative_embeddings, positive_embeddings], dim=1)\n",
        "            \n",
        "        return batch_X.to(device), batch_features.to(device), batch_y.to(device)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.y)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AF1WWWD0oTu"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional import accuracy\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "def prepare_dataloader(X, y):\n",
        "  dataset = BatchIndexedDataset(X, y)        \n",
        "  sampler = data.sampler.BatchSampler(\n",
        "      data.sampler.RandomSampler(dataset),\n",
        "      batch_size=CFG['batch_size'],\n",
        "      drop_last=False)\n",
        "  \n",
        "  return data.DataLoader(dataset, sampler=sampler, batch_size=None)\n",
        "\n",
        "def evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y):\n",
        "    \"\"\" Train classifier \"\"\"\n",
        "    train_loader = prepare_dataloader(train_X, train_y)\n",
        "    val_loader = prepare_dataloader(dev_X, dev_y)\n",
        "    test_loader = prepare_dataloader(test_X, test_y)\n",
        "\n",
        "    feature_num = next(iter(val_loader))[1].size(-1)\n",
        "    model = HateClassifier(2, feature_num=feature_num).to(device)\n",
        "\n",
        "    tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        save_top_k=1,\n",
        "        monitor='valid_loss',\n",
        "        mode='min'\n",
        "    )\n",
        "    \n",
        "    trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0, max_epochs=CFG['epochs'], progress_bar_refresh_rate=20,\n",
        "                        profiler=\"simple\", checkpoint_callback=checkpoint_callback)\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    \n",
        "    checkpoint = torch.load(checkpoint_callback.best_model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    \n",
        "    test_probabs = [] \n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "      for batch_text_X, batch_features, batch_text_y in test_loader:\n",
        "        test_probabs.append(model(batch_text_X, batch_features))\n",
        "        true_labels.extend(batch_text_y.to(device).flatten().tolist())\n",
        "\n",
        "    test_probabs = torch.cat(test_probabs, dim=0)\n",
        "    test_predictions  = test_probabs.argmax(dim=1)\n",
        "\n",
        "    y_true = np.array(true_labels).flatten()\n",
        "    y_pred = test_predictions.tolist() \n",
        "\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    result_dict = classification_report(y_true, y_pred, output_dict=True)\n",
        "\n",
        "    print('Confusion matrix:')\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "class HateClassifier(pl.LightningModule):\n",
        "    def __init__(self, classes_num=2, feature_num=100):\n",
        "        super().__init__()\n",
        "        self.model = Net(classes_num=classes_num, feature_num=feature_num).to(device)\n",
        "        self.train_acc = pl.metrics.Accuracy()\n",
        "        self.valid_acc = pl.metrics.Accuracy()\n",
        "        self.train_f1 = pl.metrics.F1(1,average=None)\n",
        "        self.valid_f1 = pl.metrics.F1(1, average=None)\n",
        "        self.valid_conf = pl.metrics.ConfusionMatrix(2)\n",
        "\n",
        "    def forward(self, x, features):\n",
        "        x = self.model(x, features)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, features, y = batch\n",
        "        y = y.flatten()\n",
        "        output = self.forward(x, features)\n",
        "        loss = nn.CrossEntropyLoss(torch.tensor(CFG['class_weights']).to(device))(output, y)\n",
        "        self.log('train_loss',  loss, on_epoch=True)\n",
        "        self.log('train_acc', self.train_acc(output, y), prog_bar=True)\n",
        "        self.log('train_f1', self.train_f1(output, y), prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_epoch_end(self, outs):\n",
        "        epoch_acc = self.train_acc.compute()\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, features, y = batch\n",
        "        y = y.flatten()\n",
        "        output = self.forward(x, features)\n",
        "        loss = nn.CrossEntropyLoss(torch.tensor(CFG['class_weights']).to(device))(output, y)\n",
        "\n",
        "        self.log('valid_loss', loss)\n",
        "        self.log('valid_acc', self.valid_acc(output, y), prog_bar=True)\n",
        "        self.log('valid_f1', self.valid_f1(output, y), prog_bar=True)\n",
        "        self.log('valid_conf', self.valid_conf(output, y))\n",
        "        \n",
        "        return {'loss': loss, 'true_labels': output, 'predictions': y}\n",
        "\n",
        "    def validation_epoch_end(self, outs):\n",
        "        val_epoch_acc = self.valid_acc.compute()\n",
        "        self.valid_f1.compute()\n",
        "        self.valid_conf.compute()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=CFG['lr'])\n",
        "        return optimizer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGf77QHR8Tjb"
      },
      "source": [
        "## Personal Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exh9Qev_Rq9W"
      },
      "source": [
        "rev_id_idx_dict = aggression_data.loc[:, ['rev_id']].reset_index().set_index('rev_id').to_dict()['index']\n",
        "worker_id_idx_dict = aggression_worker_demographics.loc[:, ['worker_id']].reset_index().set_index('worker_id').to_dict()['index']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3hx7DA6TiY"
      },
      "source": [
        "train_X = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'train'].rev_id.values)].loc[:, ['rev_id', 'worker_id']]\n",
        "dev_X = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'dev'].rev_id.values)].loc[:, ['rev_id', 'worker_id']]\n",
        "test_X = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'test'].rev_id.values)].loc[:, ['rev_id', 'worker_id']]\n",
        "\n",
        "train_y = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'train'].rev_id.values)].aggression\n",
        "dev_y = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'dev'].rev_id.values)].aggression\n",
        "test_y = aggression_annotations.loc[aggression_annotations.rev_id.isin(aggression_data[aggression_data.split == 'test'].rev_id.values)].aggression\n",
        "\n",
        "for df in [train_X, dev_X, test_X]:\n",
        "  df['worker_id'] = df['worker_id'].apply(lambda w_id: worker_id_idx_dict[w_id])\n",
        "  df['rev_id'] = df['rev_id'].apply(lambda r_id: rev_id_idx_dict[r_id])\n",
        "\n",
        "train_X, dev_X, test_X, train_y, dev_y, test_y = train_X.values, dev_X.values, test_X.values, train_y.values, dev_y.values, test_y.values"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVV88cq002EX"
      },
      "source": [
        "train_rev_ids = aggression_data[aggression_data.split == 'train'].rev_id.to_list()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9B-6bi4YtSE"
      },
      "source": [
        "annotator_negative_embeddings = torch.zeros(len(worker_id_idx_dict.keys()), 300)\n",
        "annotator_positive_embeddings = torch.zeros(len(worker_id_idx_dict.keys()), 300)\n",
        "\n",
        "worker_annotations = aggression_annotations[aggression_annotations.rev_id.isin(train_rev_ids)].groupby(['worker_id', 'aggression'])['rev_id'].apply(list).to_dict()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K_5c1y2YMdh"
      },
      "source": [
        "for i in worker_id_idx_dict.keys():\n",
        "  if (i, 0.0) in worker_annotations:\n",
        "    negative_text_idxs = [rev_id_idx_dict[r_idx] for r_idx in worker_annotations[(i, 0.0)]]\n",
        "    annotator_negative_embeddings[worker_id_idx_dict[i]] = all_embeddings[negative_text_idxs].mean(axis=0)\n",
        "  if (i, 1.0) in worker_annotations:\n",
        "    positive_text_idxs = [rev_id_idx_dict[r_idx] for r_idx in worker_annotations[(i, 1.0)]]\n",
        "    annotator_positive_embeddings[worker_id_idx_dict[i]] = all_embeddings[positive_text_idxs].mean(axis=0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_NGG45e2tnF"
      },
      "source": [
        "## S1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scfOh76FJxDR"
      },
      "source": [
        "CFG = {\n",
        "    'lr': 7*1e-4, \n",
        "    'epochs': 30,\n",
        "    'class_weights': [1.0, 1.0],\n",
        "    'batch_size': 3000,\n",
        "    'scenario': 's1'\n",
        "}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdQuh0wn4FKZ",
        "scrolled": true
      },
      "source": [
        "results_s1 = {}\n",
        "for i in range(10):\n",
        "  results_s1[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4snRUzs2vVl"
      },
      "source": [
        "## S2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMIjrfgY2yG0"
      },
      "source": [
        "CFG = {\n",
        "    'lr': 7*1e-4, \n",
        "    'epochs': 30,\n",
        "    'class_weights': [1.0, 1.0],\n",
        "    'batch_size': 3000,\n",
        "    'scenario': 's2'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrryoOGg229i"
      },
      "source": [
        "results_s2 = {}\n",
        "for i in range(10):\n",
        "  results_s2[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvdJDskSBncS"
      },
      "source": [
        "## S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7zwiO1GBo2w"
      },
      "source": [
        "CFG = {\n",
        "    'lr': 7*1e-4, \n",
        "    'epochs': 30,\n",
        "    'class_weights': [1.0, 1.0],\n",
        "    'batch_size': 3000,\n",
        "    'scenario': 's3'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW2HOBe1Bqzr"
      },
      "source": [
        "results_s3 = {}\n",
        "for i in range(10):\n",
        "  results_s3[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v36pcQ5Jfy5s"
      },
      "source": [
        "## S4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af-lDK5wf0Am"
      },
      "source": [
        "CFG = {\n",
        "    'lr': 7*1e-4, \n",
        "    'epochs': 30,\n",
        "    'class_weights': [1.0, 1.0],\n",
        "    'batch_size': 3000,\n",
        "    'scenario': 's4'\n",
        "}"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EatfcHSIf2Gf"
      },
      "source": [
        "results_s4 = {}\n",
        "for i in range(10):\n",
        "  results_s4[i] = evaluate(train_X, dev_X, test_X, train_y, dev_y, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XegtNefXalio",
        "outputId": "0d0b9aed-7679-46dd-c1fa-2361a474f1a2"
      },
      "source": [
        "def get_mean_results(results):\n",
        "  accuracy = np.mean([results[i]['accuracy'] for i in results.keys()])\n",
        "  precision_macro = np.mean([results[i]['macro avg']['precision'] for i in results.keys()])\n",
        "  recall_macro = np.mean([results[i]['macro avg']['recall'] for i in results.keys()])\n",
        "  f1_macro = np.mean([results[i]['macro avg']['f1-score'] for i in results.keys()])\n",
        "  precision_a = np.mean([results[i]['1']['precision'] for i in results.keys()])\n",
        "  recall_a = np.mean([results[i]['1']['recall'] for i in results.keys()])\n",
        "  f1_a = np.mean([results[i]['1']['f1-score'] for i in results.keys()])\n",
        "\n",
        "  return {'accuracy': accuracy, \n",
        "          'precision_macro': precision_macro,\n",
        "          'recall_macro': recall_macro,\n",
        "          'f1_macro': f1_macro,\n",
        "          'precision_a': precision_a,\n",
        "          'recall_a': recall_a,\n",
        "          'f1_a': f1_a,\n",
        "          }\n",
        "\n",
        "print('S1')\n",
        "print(get_mean_results(results_s1))\n",
        "\n",
        "print('S2')\n",
        "print(get_mean_results(results_s2))\n",
        "\n",
        "print('S3')\n",
        "print(get_mean_results(results_s3))\n",
        "\n",
        "print('S4')\n",
        "print(get_mean_results(results_s4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "S1\n",
            "{'accuracy': 0.8758207873944442, 'precision_macro': 0.8344479242787705, 'recall_macro': 0.7282863345523454, 'f1_macro': 0.763803514367855, 'precision_a': 0.7799252955895319, 'recall_a': 0.4893098878212713, 'f1_a': 0.601147586042098}\n",
            "S2\n",
            "{'accuracy': 0.8778048946141134, 'precision_macro': 0.8372304028080834, 'recall_macro': 0.7333551082017212, 'f1_macro': 0.7687231011365694, 'precision_a': 0.7835378740841303, 'recall_a': 0.4993752070809864, 'f1_a': 0.6098911747838397}\n",
            "S3\n",
            "{'accuracy': 0.892175861990899, 'precision_macro': 0.8512249115066648, 'recall_macro': 0.7767988339160098, 'f1_macro': 0.8059660308716934, 'precision_a': 0.7938998592219725, 'recall_a': 0.5899110143418376, 'f1_a': 0.6766348597055765}\n",
            "S4\n",
            "{'accuracy': 0.8847873038871658, 'precision_macro': 0.8398945068894392, 'recall_macro': 0.7596638220651589, 'f1_macro': 0.7900044278595199, 'precision_a': 0.7779806523717221, 'recall_a': 0.5569886874615421, 'f1_a': 0.648927872698743}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNZetxyV7LW9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}